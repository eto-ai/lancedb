{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b5d346-2c2a-4341-a132-00e53543f8d1",
   "metadata": {},
   "source": [
    "# The Disappearing Embedding Function\n",
    "\n",
    "Previously, to use vector databases, you had to do the embedding process yourself and interact with the system using vectors directly.\n",
    "With this new release of LanceDB, we make it much more convenient so you don't need to worry about that at all.\n",
    "\n",
    "1. We present you with sentence-transformer, openai, and openclip embedding functions that can be saved directly as table metadata\n",
    "2. You no longer have to generate the vectors directly either during query time or ingestion time\n",
    "3. The embedding function interface is extensible so you can create your own\n",
    "4. The function is persisted as table metadata so you can use it across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c25eb9d-9e05-4133-927e-747516cb9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4bd459-9bab-4803-bbe8-20201b445245",
   "metadata": {},
   "source": [
    "## Multi-modal search made easy\n",
    "\n",
    "In this example we'll multi-modal image search using:\n",
    "- Oxford Pet dataset\n",
    "- OpenClip model\n",
    "- LanceDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f56d3-0794-4018-a397-6a8f3e1b0050",
   "metadata": {},
   "source": [
    "First, download the dataset from https://www.robots.ox.ac.uk/~vgg/data/pets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dae94b-ad78-41d4-aa45-06cb96a0fff1",
   "metadata": {},
   "source": [
    "### Define embedding function\n",
    "\n",
    "We'll use the OpenClipEmbeddingFunction here for multi-modal image search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bcd5f5-29a2-4b81-9262-852ef456db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.embeddings import EmbeddingFunctionRegistry\n",
    "\n",
    "registry = EmbeddingFunctionRegistry.get_instance()\n",
    "clip = registry.get(\"openclip\").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96cbca-3dbf-4934-81b7-f8836e18509f",
   "metadata": {},
   "source": [
    "### Declare the table schema\n",
    "\n",
    "pydantic makes it really easy to bridge python and our database schema.\n",
    "\n",
    "We'll declare a new model that subclasses LanceModel to represent the table.\n",
    "This table has two columns, corresponding to the ones defined above in the embedding function.\n",
    "The embedding function defines the number of dimensions in its vectors so you don't need to \n",
    "look it up.\n",
    "\n",
    "Here because we're working with images, we add a convenience method to open the image and return a PIL Image so it can be visualized in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e7f4b-788a-4396-9c79-7e5ede47e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "class Pets(LanceModel):\n",
    "    vector: Vector(clip.ndims) = clip.VectorField()\n",
    "    image_uri: str = clip.SourceField()\n",
    "    \n",
    "    @property\n",
    "    def image(self):\n",
    "        return Image.open(self.image_uri)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b54c67-0ee0-47e7-8b72-97fec7ce4140",
   "metadata": {},
   "source": [
    "### Create the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4b38a-4636-4ad7-b3ca-0aa46aba6afc",
   "metadata": {},
   "source": [
    "First we connect to a local lancedb directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68cff2-0fdb-4748-ba4d-5e65e5a2b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect(\"~/.lancedb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf5a63-8217-4110-8aa9-42946c1a0026",
   "metadata": {},
   "source": [
    "Next we get all of the paths for the images we downloaded and create a table.\n",
    "Notice that we didn't have to worry about generating the image embeddings ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196dfc99-aa6e-48a2-a7ca-c1e2db1ac674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "\n",
    "if \"pets\" in db:\n",
    "    table = db[\"pets\"]\n",
    "else:\n",
    "    table = db.create_table(\"pets\", schema=Pets)\n",
    "    # use a sampling of 1000 images\n",
    "    p = Path(\"~/Downloads/images\").expanduser()\n",
    "    uris = [str(f) for f in p.iterdir()]\n",
    "    uris = sample(uris, 1000)\n",
    "    table.add(pd.DataFrame({\"image_uri\": uris}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db8145-04b1-40de-8b9d-09eae5c77d24",
   "metadata": {},
   "source": [
    "### Querying via text\n",
    "\n",
    "We also don't need to generate the embeddings when querying either.\n",
    "LanceDB does that automatically so you can query directly using text input.\n",
    "\n",
    "The pydantic model we declared for the table schema also makes it really easy for us to work with the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c6664-638f-4c9a-be18-434e38168061",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = table.search(\"dog\").limit(3).to_pydantic(Pets)\n",
    "rs[0].image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bed78e-0c3f-498e-afe1-344979a8ed5f",
   "metadata": {},
   "source": [
    "### Querying via images\n",
    "\n",
    "The great thing about CLIP is that it's multi-modal.\n",
    "So you can search using not just text but images as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898dbc3-57bd-4899-b825-b9b3d4652531",
   "metadata": {},
   "source": [
    "Create a query image using PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc886e8-c6bc-4db9-a4ad-02fdb1a9b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "p = Path(\"/Users/changshe/Downloads/images/samoyed_100.jpg\")\n",
    "query_image = Image.open(p)\n",
    "query_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca1269-755f-4b8f-b86d-d18e956a7cb6",
   "metadata": {},
   "source": [
    "Pass in the query_image to the search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefdd03-dc28-4827-a44e-27522689448f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rs = table.search(query_image).limit(3).to_pydantic(Pets)\n",
    "rs[0].image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193835d9-e9c7-41ae-b2e1-4edda989b87d",
   "metadata": {},
   "source": [
    "### Persistence\n",
    "\n",
    "Embedding functions are persisted as table metadata so it's much easier to use across sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732399d-7850-4d67-9f42-f60e1629861e",
   "metadata": {},
   "source": [
    "For example we can recreate the database connection and table object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed91086-cecd-4acf-893f-200b9e3099dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect(\"~/.lancedb\")\n",
    "table = db[\"pets\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807fd59-48dd-419b-8c3e-705ccf63e077",
   "metadata": {},
   "source": [
    "We can observe that it's read out as table metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53156aa2-be02-4c80-9a54-b66f9f0ee7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(table.schema.metadata[b\"embedding_functions\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b64e40-8364-4efb-a0e6-2692af946f7c",
   "metadata": {},
   "source": [
    "And we can also run queries as before without having to reinstantiate the embedding function explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c994859-31f1-41cb-ae86-8f41ddc707f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = table.search(\"big dog\").limit(3).to_pydantic(Pets)\n",
    "rs[0].image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a44b94-ca9e-42c4-82fc-73ab2f3cf424",
   "metadata": {},
   "source": [
    "## LanceDB makes multimodal AI easy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b92d6-b666-49ba-ba46-d7a5ac077c6f",
   "metadata": {},
   "source": [
    "- LanceDB's new embedding functions feature makes it easy for builders of LLM apps\n",
    "- You no longer need to manually encode the data yourself\n",
    "- You no longer need to figure out how many dimensions is your vector\n",
    "- You no longer need to manually encode the query\n",
    "- And with the right embedding model, you can search way more than just text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96da468-d20d-4a18-9a58-a5d99789e5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
